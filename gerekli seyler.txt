#Gerekli seyler
for app in rke2; do   output=$(curl -ks "https://update.${app}.io/v1-release/channels" | jq --arg app "${app}" -r '.data[]|select(.id==("stable","latest","testing"))|[$app, .name, .latest]|@tsv');   [ -n "$output" ] && echo "$output"; done && echo
piserver pod logs (in /var/log/pods)
kubectl rollout restart ds/rke2-canal -n kube-system
kubectl rollout restart deployment/rancher -n cattle-system
kubectl delete pod -l k8s-app=canal -n kube-system
tail -f /var/lib/rancher/rke2/agent/containerd/containerd.log

kubectl get pod -n kube-system -l tier=control-plane -o wide

kubelet logs /var/lib/rancher/rke2/agent/log


kubectl patch pvc <pvc_name> -p '{"metadata":{"finalizers":null}}'

kubectl delete pvc <pvc_name> --grace-period=0 --force 

kubectl patch pv <pv_name> -p '{"metadata":{"finalizers":null}}'

kubectl delete pv <pv_name> --grace-period=0 --force 

127.0.0.1 rke21
10.0.0.3 rke22
10.0.0.4 rke23
10.0.0.5 rke2agent1
10.0.0.6 rke2agent2


127.0.0.1               localhost.localdomain    localhost	
::1                     localhost6.localdomain6    localhost6
# cluster nodes.
10.0.0.2          rke21.rancher.exemple.com    rke21
10.0.0.3          rke22.rancher.exemple.com    rke22
10.0.0.4          rke23.rancher.exemple.com    rke23
10.0.0.5          rke2agent1.rancher.exemple.com    rke2agent1
10.0.0.6          rke2agent2.rancher.exemple.com    rke2agent2


kubectl delete -n cattle-system MutatingWebhookConfiguration rancher.cattle.io //// Internal error occurred: failed calling webhook "rancher.cattle.io.clusters.management.cattle.io": failed to call webhook: Post "https://rancher-webhook.cattle-system.svc:443/v1/webhook/mutation/clusters.management.cattle.io?timeout=10s": no endpoints available for service "rancher-webhook"
then re-run helm install/upgrade rancher-helm-chart, it will re-create these webhooks.

this error will happen if you had installed rancher helm chart before and the rancher webhook pod not up running when you update cluster. because rancher has installed two webhooks, you can check them by below commands

kubectl get -n cattle-system MutatingWebhookConfiguration rancher.cattle.io
kubectl get -n cattle-system validatingwebhookconfigurations rancher.cattle.io

in some cases, rancher webhook pod is not up running when upgrade cluster. you can run below to delete them.

kubectl delete -n cattle-system MutatingWebhookConfiguration rancher.cattle.io
kubectl delete -n cattle-system validatingwebhookconfigurations rancher.cattle.io

then re-run helm install/upgrade rancher-helm-chart, it will re-create these webhooks.
##### etcd speak
kubectl exec -it etcd-rke21 -n kube-system --  etcdctl --cert /var/lib/rancher/rke2/server/tls/etcd/server-client.crt --key /var/lib/rancher/rke2/server/tls/etcd/server-client.key --endpoints https://127.0.0.1:2379 --cacert /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt member list
###
kubectl exec -it etcd-rke21 -n kube-system --  etcdctl --cert /var/lib/rancher/rke2/server/tls/etcd/server-client.crt --key /var/lib/rancher/rke2/server/tls/etcd/server-client.key --endpoints https://127.0.0.1:2379 --cacert /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt endpoint health
##
kubectl exec -it etcd-rke21 -n kube-system --  etcdctl --cert /var/lib/rancher/rke2/server/tls/etcd/server-client.crt --key /var/lib/rancher/rke2/server/tls/etcd/server-client.key --endpoints https://127.0.0.1:2379 --cacert /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt endpoint status
##
To get all etcd endpoint status
for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do kubectl exec -it $etcdpod -n kube-system --  etcdctl --cert /var/lib/rancher/rke2/server/tls/etcd/server-client.crt --key /var/lib/rancher/rke2/server/tls/etcd/server-client.key --endpoints https://127.0.0.1:2379 --cacert /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt endpoint status; done


unset ETCDCTL_ENDPOINTS
export ETCDCTL_API=3
export ETCDCTL_CACERT=/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt
export ETCDCTL_CERT=/var/lib/rancher/rke2/server/tls/etcd/server-client.crt
export ETCDCTL_KEY=/var/lib/rancher/rke2/server/tls/etcd/server-client.key

etcdctl --endpoints=https://127.0.0.1:2379 endpoint status

&& see domain of certificate
openssl x509 -in /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt -text -noout
 curl -v -k  https://127.0.0.1:9345/v1-rke2/readyz
 
 # /etc/rancher/rke2/config.yaml
kubelet-arg:
 - "cpu-manager-policy=static"
 - "cpu-manager-reconcile-period=5s"
 - "topology-manager-policy=best-effort"
 - "topology-manager-scope=container"
 - "kube-reserved=cpu=500m,memory=1Gi,ephemeral-storage=1Gi"
 - "system-reserved=cpu=500m,memory=1Gi,ephemeral-storage=1Gi"
 - "reserved-system-cpus=0,1"
- "eviction-hard=memory.available<500Mi,nodefs.available<1Gi,imagefs.available<15Gi"
 - "eviction-soft=memory.available<1Gi,nodefs.available<5Gi,imagefs.available<20Gi"
 - "eviction-soft-grace-period=memory.available=1m,nodefs.available=1m,imagefs.available=1m"
 
 
 What is pod disruption budget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: myapp

 openssl x509 -in /var/lib/rancher/rke2/agent/client-kube-proxy.crt -noout -enddate
 
 
 
 Try adding --kubelet-insecure-tls

kubectl edit deploy metrics-server -n kube-system

      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=8448
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-insecure-tls
		

Also if the --kubelet-insecure-tls did not fixed it, you might need to extend the time when it check for ready ( this was an issue I had )

Had to change to:

          initialDelaySeconds: 300
          periodSeconds: 30


if [[ $(timedatectl status | grep -i "NTP service" | awk '{print $3}') == "active" ]]; then echo "NTP is running"; else echo "NTP is not running"; fi

curl --cacert /var/lib/rancher/rke2/server/tls/server-ca.crt \
     --cert /var/lib/rancher/rke2/server/tls/client-admin.crt \
     --key /var/lib/rancher/rke2/server/tls/client-admin.key \
     "https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=5s"
	
	

curl --cacert /var/lib/rancher/rke2/server/tls/server-ca.crt \
     --cert /var/lib/rancher/rke2/server/tls/client-admin.crt \
     --key /var/lib/rancher/rke2/server/tls/client-admin.key \
     "https://127.0.0.1:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s"	
	
	
kubectl get --raw='/healthz'
journalctl -u rke2-server